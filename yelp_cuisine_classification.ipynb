{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1e64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from math import log\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# feature engineering libraries\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# model building & evaluating libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from gensim.models import word2vec\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "import keras.backend as K\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f95fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # get English stopwords\n",
    "    en = spacy.load('en_core_web_sm')\n",
    "    sw_spacy = en.Defaults.stop_words\n",
    "    stop_words = set(sw_spacy)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # stop_words.add('would')\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"].lower()\n",
    "        sent = sent.replace('\\\\n',' ')\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d026ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0f1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train['name'] + ' ' + df_train[\"review\"]\n",
    "df_test[\"text\"] = df_test['name'] + ' ' + df_test[\"review\"]\n",
    "\n",
    "df_train = preprocess_df(df_train)\n",
    "df_test = preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3facf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_train[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f522ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b19e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_all[\"text\"])]\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f26c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-229e16b082ed>:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "# GloVE\n",
    "# A function used to learn word embeddings through Word2vec module\n",
    "def get_embeddings_g(inp_data, vocabulary_inv):\n",
    "    glove_file = 'glove.6B.300d.txt'\n",
    "    tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "    _ = glove2word2vec(glove_file, tmp_file)\n",
    "    embedding_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), 300))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model:\n",
    "            embedding_weights[i] = embedding_model[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "    return embedding_weights\n",
    "\n",
    "embedding_weights_g = get_embeddings_g(inp_data, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d626141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe\n",
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        vec += embedding_weights_g[vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "    \n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            vec += embedding_weights_g[vocabulary[w]]\n",
    "            length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)\n",
    "    \n",
    "param_grid = [\n",
    "    {\n",
    "        'C' : np.linspace(1, 101, 10, endpoint=False)\n",
    "    }\n",
    "]\n",
    "\n",
    "# clf_g = GridSearchCV(LogisticRegression(max_iter=100000000), param_grid = param_grid, cv = 5, n_jobs=-1, scoring = 'f1_weighted').fit(train_vec, df_train[\"label\"].values.ravel())\n",
    "# print(clf_g.best_params_)\n",
    "# print(clf_g.best_score_)\n",
    "\n",
    "clf_g = LogisticRegression(max_iter=100000000, C = 21).fit(train_vec, df_train[\"label\"].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5705fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_g = clf_g.predict(test_vec)\n",
    "prob_g = clf_g.predict_proba(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e3c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(pred_g):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "df_g = pd.DataFrame.from_dict(dic)\n",
    "df_g.to_csv('predicted_g.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b3f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "\n",
    "def get_embeddings_w(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=10):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15\n",
    "    downsampling = 1e-3\n",
    "    print('Training Word2Vec model...')\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b29dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "15 1 {'C': 61.0} 0.809245233368571\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "15 2 {'C': 51.0} 0.809737415872515\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "15 3 {'C': 21.0} 0.8096562457063345\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "16 1 {'C': 51.0} 0.8106870319515561\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "16 2 {'C': 31.0} 0.8112296457142367\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "16 3 {'C': 31.0} 0.812320346728808\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "17 1 {'C': 31.0} 0.8121444701548975\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "17 2 {'C': 41.0} 0.8113741901675651\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "17 3 {'C': 41.0} 0.8120718678348939\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "18 1 {'C': 31.0} 0.8114342599586175\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "18 2 {'C': 31.0} 0.8119085136716521\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "18 3 {'C': 21.0} 0.8118824030186657\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "19 1 {'C': 11.0} 0.809613098986514\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "19 2 {'C': 21.0} 0.8121168655096263\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "19 3 {'C': 21.0} 0.8118534675059225\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "20 1 {'C': 31.0} 0.8142067377671044\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n",
      "20 2 {'C': 51.0} 0.8103667678944785\n",
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 3 {'C': 31.0} 0.8134123597494236\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec - sg\n",
    "\n",
    "for i in range(15,21): # context window\n",
    "    for j in range(1,4): # min word count\n",
    "        \n",
    "        embedding_weights_w = get_embeddings_w(inp_data, vocabulary_inv, min_word_count = j, context = i)\n",
    "\n",
    "        train_vec = []\n",
    "        for doc in tagged_train_data:\n",
    "            vec = 0\n",
    "            for w in doc:\n",
    "                vec += embedding_weights_w[vocabulary[w]]\n",
    "            vec = vec / len(doc)\n",
    "            train_vec.append(vec)\n",
    "\n",
    "        test_vec = []\n",
    "        for doc in tagged_test_data:\n",
    "            vec = 0\n",
    "            length = 0\n",
    "            for w in doc:\n",
    "                try:\n",
    "                    vec += embedding_weights_w[vocabulary[w]]\n",
    "                    length += 1\n",
    "                except:\n",
    "                    continue\n",
    "            vec = vec / length\n",
    "            test_vec.append(vec)\n",
    "\n",
    "        param_grid = [\n",
    "            {\n",
    "                'C' : np.linspace(1, 101, 10, endpoint=False)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        clf_w = GridSearchCV(LogisticRegression(max_iter=100000000), param_grid = param_grid, cv = 5, n_jobs=-1, scoring = 'f1_weighted').fit(train_vec, df_train[\"label\"].values.ravel())\n",
    "        print(i, j, clf_w.best_params_, clf_w.best_score_)\n",
    "\n",
    "# clf_w = LogisticRegression(max_iter=100000000, C = 21).fit(train_vec, df_train[\"label\"].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92ba11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "# embedding_weights_w = get_embeddings_w(inp_data, vocabulary_inv, min_word_count = 2, context = 15) # 1st GSCV best\n",
    "embedding_weights_w = get_embeddings_w(inp_data, vocabulary_inv, min_word_count = 3, context = 20) # 2nd GSCV best\n",
    "\n",
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        vec += embedding_weights_w[vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "\n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            vec += embedding_weights_w[vocabulary[w]]\n",
    "            length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)\n",
    "\n",
    "clf_w = LogisticRegression(max_iter=100000000, C = 31).fit(train_vec, df_train[\"label\"].values.ravel())\n",
    "\n",
    "pred_w = clf_w.predict(test_vec)\n",
    "# prob_w = clf_w.predict_proba(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a17da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(pred_w):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "df_w = pd.DataFrame.from_dict(dic)\n",
    "df_w.to_csv('predicted_w1.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f064073",
   "metadata": {},
   "source": [
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "5 1 {'C': 41.0} 0.794418508196163\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "5 2 {'C': 21.0} 0.7937145951885695\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "5 3 {'C': 41.0} 0.792459518229485\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "5 4 {'C': 41.0} 0.7934537644782814\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "6 1 {'C': 41.0} 0.7951942195718269\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "6 2 {'C': 31.0} 0.7971248511460478\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "6 3 {'C': 51.0} 0.7990957917934778\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "6 4 {'C': 21.0} 0.7956507139877707\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "7 1 {'C': 41.0} 0.7996975203916448\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "7 2 {'C': 41.0} 0.7985956444448835\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "7 3 {'C': 21.0} 0.7991956066181962\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "7 4 {'C': 31.0} 0.8027958423442868\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "8 1 {'C': 21.0} 0.8012539656394286\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "8 2 {'C': 31.0} 0.8011456621451426\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "8 3 {'C': 21.0} 0.8007228701901392\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "8 4 {'C': 51.0} 0.8026179260355658\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "9 1 {'C': 21.0} 0.8022144678896608\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "9 2 {'C': 31.0} 0.8031696965737642\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "9 3 {'C': 31.0} 0.8030696225093106\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "9 4 {'C': 81.0} 0.8028727668991078\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "10 1 {'C': 41.0} 0.8038862976735344\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "10 2 {'C': 71.0} 0.8048346583437352\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "10 3 {'C': 31.0} 0.8043503317835073\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "10 4 {'C': 31.0} 0.8035541278403127\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "11 1 {'C': 41.0} 0.8065086348753641\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "11 2 {'C': 51.0} 0.8060389118465444\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "11 3 {'C': 41.0} 0.8060117035387135\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "11 4 {'C': 31.0} 0.8078045853227838\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "12 1 {'C': 31.0} 0.807447670766912\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "12 2 {'C': 31.0} 0.8042719652105946\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "12 3 {'C': 31.0} 0.8088277928438016\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "12 4 {'C': 31.0} 0.8075353561485071\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "13 1 {'C': 41.0} 0.8078391339565909\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "13 2 {'C': 51.0} 0.8076055297955644\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "13 3 {'C': 51.0} 0.8092122784390974\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "13 4 {'C': 21.0} 0.8094441756253264\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "14 1 {'C': 61.0} 0.8091134123624986\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "14 2 {'C': 21.0} 0.808784782430904\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "14 3 {'C': 41.0} 0.8098692651548213\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "14 4 {'C': 31.0} 0.8070831011781012\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "15 1 {'C': 51.0} 0.8104231315171668\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "15 2 {'C': 41.0} 0.812373288681249\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "15 3 {'C': 21.0} 0.8105128540071302\n",
    "Training Word2Vec model...\n",
    "Model: skip-gram\n",
    "Saving Word2Vec model embedding\n",
    "15 4 {'C': 41.0} 0.8113853300138688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d8dc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(prob_w).to_csv('best_w2v_probs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ae74708",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(prob_g).to_csv('best_glv_probs.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
